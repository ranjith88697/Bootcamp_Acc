{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F96VRX22-IkX"
   },
   "source": [
    "#**Create local vector embeddings using sentens-transformer python library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGYeMFL2bojw"
   },
   "source": [
    "##**GOAL: to embed text sentences and perform semantic searches using your own Python code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOPrcTxuFWJl"
   },
   "source": [
    "There are many pre-trained embedding models available on Hugging Face that you can use to create vector embeddings.\n",
    "Sentence Transformers (SBERT) is a library that makes it easy to use these models for vector embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsqP-DgCGHcB"
   },
   "source": [
    "Use pip  to install  'sentence_transformers' library  and import  'SentenceTransformer model loader' from this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.21.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: typer>=0.23.1 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY-ID8AfHKYX"
   },
   "source": [
    "Load the 'paraphrase-MiniLM-L6-v2' model  from HuggingFace resource  using the  SentenceTransformer( *model-name* )  and store the reference to the model object in the 'model' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f1cf5a080e4d19b6fd650ea97be8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87f47dc93f549e2acd1e916af241b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb9148b41aa48369887227b241f0dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00938b5cf37482b9c79a4d78cab98b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8df6014f0294b3399914fc7e8efec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76089dc6ded84d6f8876030f704c0647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554118b086544b10a81c1f5d935ccd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ad343061044545b9eedbf4b83153f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8ecc2a80354cefa5de47aae9ef13aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124f3cd49b94408fa732e7865704995d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2a217c7c244927976f7d84acaab42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6164136554475faa80a57929c5561a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6pgQE3aMkK_"
   },
   "source": [
    "After loading the model, call the 'encode()' method on the model object to create a vector representation of a specific text sentence. Use your own text string  as the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0168511 , -0.07072181,  0.08554108, ...,  0.09888572,\n",
       "         0.01277482, -0.08485193],\n",
       "       [-0.00446599, -0.0631337 ,  0.06713641, ...,  0.10224469,\n",
       "         0.02390005, -0.07414539],\n",
       "       [ 0.13023719, -0.01577286, -0.0367167 , ...,  0.05145747,\n",
       "         0.00296658,  0.05249828]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete the code\n",
    "sentence = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Artificial intelligence is fascinating.\",\n",
    "    \"The cat sat on the mat.\"\n",
    "]\n",
    "\n",
    "embedding = model.encode(sentence)\n",
    "embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql9pFPX4SsnZ"
   },
   "source": [
    "Create vector representations for several text sentences. Place the text strings in a list and use this list as an argument. Use 8-10 sentences of 20-25 words each.  Call the 'encode()' method on the model object with the list of sentences as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "# complete the code\n",
    "# List of sentences\n",
    "sentences_list = [\n",
    "    \"Machine learning enables computers to identify patterns in large datasets and make predictions without being explicitly programmed for every possible scenario.\",\n",
    "    \"Natural language processing allows systems to understand human language, interpret meaning, and generate responses that feel intuitive and contextually appropriate.\",\n",
    "    \"Modern recommendation engines analyze user behavior, preferences, and historical interactions to deliver personalized suggestions across shopping platforms, streaming services, and social media.\",\n",
    "    \"Semantic search improves information retrieval by focusing on meaning rather than exact keyword matching, helping users find relevant content even when phrasing differs.\",\n",
    "    \"Neural networks consist of interconnected layers of artificial neurons that learn complex relationships through repeated exposure to labeled or unlabeled training data.\",\n",
    "    \"Vector embeddings convert text into numerical representations that capture semantic similarity, enabling efficient comparison, clustering, and retrieval of related information.\",\n",
    "    \"Transformers revolutionized deep learning by introducing attention mechanisms that allow models to focus on important parts of input sequences during processing.\",\n",
    "    \"Large language models are trained on massive corpora of text, enabling them to generate coherent responses, summarize documents, and assist with a wide range of tasks.\",\n",
    "    \"Efficient indexing techniques such as FAISS enable fast similarity search across millions of embeddings, making largeâ€‘scale retrieval systems practical and responsive.\",\n",
    "    \"Text preprocessing steps like tokenization, normalization, and cleaning help improve model performance by ensuring consistent and meaningful input representations.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences_list)\n",
    "\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIATXYYKWNSe"
   },
   "source": [
    "#**Definition of semantic textual similaritye**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "addEfKLoXawa"
   },
   "source": [
    "Import 'util' module from sentence_transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPbGmfW4YJoZ"
   },
   "source": [
    "You can calculate the cosine similarity of the vector representations of our sentences using the 'cos_sim()' function from the util module.\n",
    "Example: sim = util.cos_sim(embedding_1, embedding_2). Calculate the cosine similarity for any two sentences from your list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.5001106262207031\n"
     ]
    }
   ],
   "source": [
    "# Example sentences\n",
    "sentences_list = [\n",
    "    \"Machine learning helps computers identify patterns in data.\",\n",
    "    \"Artificial intelligence enables systems to perform tasks that normally require human intelligence.\",\n",
    "    \"The weather today is sunny with a light breeze.\"\n",
    "]\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = model.encode(sentences_list)\n",
    "\n",
    "# Pick any two sentences (e.g., 0 and 1)\n",
    "embedding_1 = embeddings[0]\n",
    "embedding_2 = embeddings[1]\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = util.cos_sim(embedding_1, embedding_2)\n",
    "\n",
    "print(\"Cosine similarity:\", similarity.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZPd8fUmazhu"
   },
   "source": [
    "Write and test a function named 'cos_similarity_calculation' that determines the semantic similarity between the sentences in your list and any text sentence using their vector representations and the cosine distance as a similarity measure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5181  ->  Machine learning helps computers identify patterns in data.\n",
      "0.7436  ->  Artificial intelligence enables systems to perform tasks requiring human reasoning.\n",
      "-0.0214  ->  The weather today is sunny with a light breeze.\n"
     ]
    }
   ],
   "source": [
    "def cos_similarity_calculation(sentences_list, query_sentence):\n",
    "    sentence_embeddings = model.encode(sentences_list)\n",
    "    query_embedding = model.encode(query_sentence)\n",
    "\n",
    "    similarities = util.cos_sim(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "    # Pair each sentence with its similarity score\n",
    "    results = list(zip(sentences_list, similarities.tolist()))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "#Example\n",
    "sentences = [\n",
    "    \"Machine learning helps computers identify patterns in data.\",\n",
    "    \"Artificial intelligence enables systems to perform tasks requiring human reasoning.\",\n",
    "    \"The weather today is sunny with a light breeze.\"\n",
    "]\n",
    "\n",
    "query = \"AI allows machines to think and make decisions.\"\n",
    "\n",
    "output = cos_similarity_calculation(sentences, query)\n",
    "\n",
    "for sentence, score in output:\n",
    "    print(f\"{score:.4f}  ->  {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ysHatbf2IdC"
   },
   "source": [
    "Create a function that determines the cosine similarity between a vector and a batch of vectors using the cosine distance formula and the numpy library. Add code to demonstrate how to use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity_single_to_batch(vector, batch_vectors):\n",
    "    # single vector\n",
    "    vector_norm = vector / np.linalg.norm(vector)\n",
    "\n",
    "    # vector in the batch\n",
    "    batch_norms = batch_vectors / np.linalg.norm(batch_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = np.dot(batch_norms, vector_norm)\n",
    "\n",
    "    return similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity scores:\n",
      "Vector 0: 0.9912\n",
      "Vector 1: 0.5442\n",
      "Vector 2: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# Example vectors (pretend these came from an embedding model)\n",
    "query_vec = np.array([0.2, 0.5, 0.3])\n",
    "batch_vecs = np.array([\n",
    "    [0.1, 0.4, 0.2],\n",
    "    [0.9, 0.1, 0.3],\n",
    "    [0.2, 0.5, 0.31]\n",
    "])\n",
    "\n",
    "scores = cosine_similarity_single_to_batch(query_vec, batch_vecs)\n",
    "\n",
    "print(\"Cosine similarity scores:\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Vector {i}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
