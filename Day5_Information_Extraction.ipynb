{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjith88697/Bootcamp_Acc/blob/main/Day5_Information_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybyz0S6sQj_T"
      },
      "source": [
        "# Information Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brzA9z-U_TBc"
      },
      "source": [
        "All assignments were tested in the [Google Colab](https://colab.research.google.com/notebooks/). All core packages are already installed there.\n",
        "\n",
        "If you use your python environment, probably you'll need to install them separately via pip or conda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQjFTtuFFhr1"
      },
      "source": [
        "## Part I. Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBstqhbBEmui"
      },
      "source": [
        "### 1.1 Basic Regex\n",
        "Write a regular expression to search for emails in text.\n",
        "\n",
        "Let's start from the basic emails like john@gmail.com\n",
        "\n",
        "<details>\n",
        "<summary>hints:</summary>\n",
        "\n",
        "you need to catch patterns like word@word.word\n",
        "\n",
        "one of the ways to catch any word character is **\\w**\n",
        "\n",
        "if you need to repeat some character you can use **+**\n",
        "\n",
        "so to catch the full word **\\w+** will be enough\n",
        "\n",
        "if you need to escape some character (because it's a reserved character), you can use **\\\\**, e.g. **\\\\.**\n",
        "\n",
        "you can find all special characters in the [documentation](https://docs.python.org/3/library/re.html)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E76zlryWCux9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def catch_simple_emails(text: str) -> list:\n",
        "\n",
        "  pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "  email_regex = re.compile(pattern, re.I)\n",
        "  emails = email_regex.findall(txt)\n",
        "  return emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrqrn4sEE4Xf",
        "outputId": "72bf3bb4-6e52-46cb-db52-b8b48a1b0729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "txt = \"Most beginners use generic free business email accounts without a domain name which isn’t very professional. For example: johnsmith2019@gmail.com or jsmithfromstargardening@yahoo.com\"\n",
        "try:\n",
        "  assert catch_simple_emails(txt) == ['johnsmith2019@gmail.com', 'jsmithfromstargardening@yahoo.com']\n",
        "  print('passed')\n",
        "except AssertionError:\n",
        "  print('failed')\n",
        "  print(catch_simple_emails(txt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alm-dBYDK4gM"
      },
      "source": [
        "### 1.2 Complex Regex\n",
        "How can it be modified to catch more complex emails like john.smith@yahoo.co.id or john-smith@yahoo.co.id?\n",
        "\n",
        "Emails also can contain other symbols but they are not required in this assignment\n",
        "<details>\n",
        "<summary>hints:</summary>\n",
        "\n",
        "you need to replace single words with word sequences separated by dots or hyphens\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VvueiNKuFOXF"
      },
      "outputs": [],
      "source": [
        "def catch_complex_emails(txt: str) -> list:\n",
        "\n",
        "  replace_txt = re.sub(r'[.-](?=[^@]*@)', '', txt)\n",
        "  #pattern = r'\\b[a-zA-Z0-9]+(?:[.-][a-zA-Z0-9]+)*@[a-zA-Z0-9]+(?:[.-][a-zA-Z0-9]+)*\\.[a-zA-Z]{2,}\\b'\n",
        "  pattern = r'[\\w.-]+@[\\w-]+(?:\\.\\w+)+'\n",
        "  email_regex = re.compile(pattern, re.I)\n",
        "  emails = email_regex.findall(txt)\n",
        "  return emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M6o61KhrGfU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d04a42-230c-4d44-d0ef-14a03047a98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "txt = \"\"\"\n",
        "simple@example.com nuff said\n",
        "very.common@example.com very.very.common\n",
        "very.common@long.example.com multiple domain levels\n",
        "other.email-with-hyphen@example.com dot + hyphen\n",
        "fully-qualified-domain@example.com hyphen only\n",
        "x@example.com single letter\n",
        "example-indeed@strange-example.com one more example\n",
        "\"\"\"\n",
        "try:\n",
        "  assert catch_complex_emails(txt) == ['simple@example.com',\n",
        "  'very.common@example.com',\n",
        "  'very.common@long.example.com',\n",
        "  'other.email-with-hyphen@example.com',\n",
        "  'fully-qualified-domain@example.com',\n",
        "  'x@example.com',\n",
        "  'example-indeed@strange-example.com']\n",
        "  print('passed')\n",
        "except AssertionError:\n",
        "  print('failed')\n",
        "  print(catch_complex_emails(txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM_lOlWtSUZl",
        "outputId": "09d14fda-cf88-4ac8-9c6e-d9b77b113e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  assert catch_complex_emails(txt) == ['simple@example.com',\n",
        "'very.common@example.com',\n",
        "'very.common@long.example.com',\n",
        "'other.email-with-hyphen@example.com',\n",
        "'fully-qualified-domain@example.com',\n",
        "'x@example.com',\n",
        "'example-indeed@strange-example.com']\n",
        "  print('passed')\n",
        "except AssertionError:\n",
        "  print('failed')\n",
        "  print(catch_complex_emails(txt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBsP3w3dWArL"
      },
      "source": [
        "### 1.3 Regex-based Tokenizer\n",
        "\n",
        "Using your knowledge of regular expressions, write a simple text tokenizer that can separate punctuation, numbers, and words into individual tokens.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "* Floating point numbers like 1.23 fit in one token. The decimal separator can be either a dot or a comma.\n",
        "* The number can be negative and have a sign like -10.5.\n",
        "* There may not be an integer part of the number at all: the sequences -0.15 and -.15 mean the same number.\n",
        "* Empty fractional part is not allowed: string \"10.\" must be split into two tokens: \"10\" and \".\"\n",
        "* Consecutive punctuation marks are separated each into a separate token.\n",
        "* Only English letters are allowed in words.\n",
        "* Empty tokens must be removed\n",
        "\n",
        "<details>\n",
        "<summary>hints:</summary>\n",
        "\n",
        "To combine individual checks into a single regex, you can use the operator | (or), e.g. '[a-z]|[0-9]' means one letter from a to z or one number from 0 to 9\n",
        "\n",
        "Remember that if regex contains multiple blocks, they are processed in natural order (from left to right), so you should catch more complex elements first (e.g., catch floating point numbers before integers); otherwise, you can lose it.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J5YCRx8bkbq2"
      },
      "outputs": [],
      "source": [
        "def text_split(txt: str) -> list:\n",
        "  pattern = r'-?\\d+(?:[.,]\\d+)?|-?\\.\\d+|[A-Za-z]+|[^A-Za-z0-9\\s]'\n",
        "  token_regex = re.compile(pattern, re.I)\n",
        "  tokens = token_regex.findall(txt)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ymJSyHPblaGE"
      },
      "outputs": [],
      "source": [
        "txt = {\n",
        "  '': [],\n",
        "  '$0.00 \"Surplus\";$-0.00 \"Shortage\"': ['$', '0.00', '\"', 'Surplus', '\"', ';', '$', '-0.00', '\"', 'Shortage', '\"'],\n",
        "  'Combine text and numbers': ['Combine', 'text', 'and', 'numbers'],\n",
        "  'Combine_text_and$$numbers': ['Combine', '_', 'text', '_', 'and', '$', '$', 'numbers'],\n",
        "  '(.4 in this example)': ['(', '.4', 'in', 'this', 'example', ')'],\n",
        "  '(4. in this example)': ['(', '4', '.', 'in', 'this', 'example', ')'],\n",
        "  '$$1,1%#-2': ['$', '$', '1,1', '%', '#', '-2'],\n",
        "  '...': ['.', '.', '.'],\n",
        "  'How to Sauté?': ['How', 'to', 'Saut', 'é', '?']\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzT6dxu8l9Ty",
        "outputId": "0e575919-1753-4f9a-c72c-f11cafd3d6ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed\n",
            "passed\n",
            "passed\n",
            "passed\n",
            "passed\n",
            "passed\n",
            "passed\n",
            "passed\n",
            "passed\n"
          ]
        }
      ],
      "source": [
        "for x in txt:\n",
        "  try:\n",
        "    assert text_split(x) == txt[x]\n",
        "    print('passed')\n",
        "  except AssertionError:\n",
        "    print('failed')\n",
        "    print(text_split(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYD-unDcs55m"
      },
      "source": [
        "## Part II. Fuzzy matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkfWi9NRDz13"
      },
      "source": [
        "Fuzzy Matching (also called Approximate String Matching) is a technique that helps identify two elements of text, strings, or entries that are approximately similar but are not exactly the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISOAKlEtPZ5"
      },
      "source": [
        "### 2.1 Levenshtein distance\n",
        "\n",
        "The Levenshtein distance is a metric used to measure the difference between 2 string sequences.\n",
        "\n",
        "It gives us a measure of the number of single character insertions, deletions or substitutions required to change one string into another.\n",
        "\n",
        " You can find an algorythm and the full description [here](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
        "\n",
        "For our task we'll use python-levenshtein library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WIPv-N2yGh22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360afa13-83c2-4189-a968-43782635ec3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.12/dist-packages (0.27.3)\n",
            "Requirement already satisfied: Levenshtein==0.27.3 in /usr/local/lib/python3.12/dist-packages (from python-levenshtein) (0.27.3)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.3->python-levenshtein) (3.14.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-levenshtein # if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KL4TrXgWtS-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5c5525-e7e5-48b4-8fd1-7f2093f4e852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance between health and Covid is 6\n",
            "distance between chiefs and Covid is 6\n",
            "distance between today and Covid is 4\n",
            "distance between announced and Covid is 7\n",
            "distance between a and Covid is 5\n",
            "distance between fresh and Covid is 5\n",
            "distance between covid and Covid is 1\n",
            "distance between vaccine and Covid is 6\n",
            "distance between programme and Covid is 8\n",
            "distance between ahead and Covid is 4\n",
            "distance between of and Covid is 4\n",
            "distance between what and Covid is 5\n",
            "distance between they and Covid is 5\n",
            "distance between fear and Covid is 5\n",
            "distance between will and Covid is 5\n",
            "distance between be and Covid is 5\n",
            "distance between a and Covid is 5\n",
            "distance between difficult and Covid is 8\n",
            "distance between winter and Covid is 6\n"
          ]
        }
      ],
      "source": [
        "# simple approach\n",
        "import string\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "txt = 'Health chiefs today announced a fresh Covid vaccine programme ahead of what they fear will be a difficult winter.'\n",
        "clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
        "words = [i for i in clean_txt.split()]\n",
        "pattern = 'Covid'\n",
        "\n",
        "for word in words:\n",
        "  print(f\"distance between {word} and {pattern} is {levenshtein_distance(word, pattern)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24vG07zHIhR_"
      },
      "source": [
        "Your task.\n",
        "Implement a function that will return the word from the text that is most similar to the given pattern.\n",
        "\n",
        "If there are many words with the same distance, please use the first one\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CE6Kf4GyGpg8"
      },
      "outputs": [],
      "source": [
        "def find_similar_word(txt: str, pattern: str) -> str:\n",
        "  import string\n",
        "  from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "  clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
        "  words = [i for i in clean_txt.split()]\n",
        "\n",
        "  if not words:\n",
        "    return ''\n",
        "\n",
        "  min_distance = float('inf')\n",
        "  out = ''\n",
        "\n",
        "  for word in words:\n",
        "    dist = levenshtein_distance(word, pattern.lower())\n",
        "    if dist < min_distance:\n",
        "      min_distance = dist\n",
        "      out = word\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "US6hul6lGpkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5109e0f-f0ee-4ae1-b8a5-d8b8caad68e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "txt = 'spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.'\n",
        "try:\n",
        "  assert find_similar_word(txt, 'dependent') == 'dependency'\n",
        "  print('passed')\n",
        "except AssertionError:\n",
        "  print('failed')\n",
        "  print(find_similar_word(txt, 'dependent'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w5sIfmFTxvU"
      },
      "source": [
        "Implement a function that will return the pair of most similar words in the given text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7J3ibh4uUjAA"
      },
      "outputs": [],
      "source": [
        "def find_similar_words(txt: str) -> list: # output should be like ['word1', 'word2']\n",
        "\n",
        "\n",
        "  clean_txt = ''.join([c for c in txt.lower() if c not in string.punctuation])\n",
        "  words = clean_txt.split()\n",
        "\n",
        "  out = ['', '']\n",
        "  best_dist = float('inf')\n",
        "\n",
        "  for i in range(len(words)):\n",
        "      for j in range(i + 1, len(words)):\n",
        "          d = levenshtein_distance(words[i], words[j])\n",
        "          if d < best_dist:\n",
        "              best_dist = d\n",
        "              best_pair = [words[i], words[j]]\n",
        "              out = best_pair\n",
        "\n",
        "\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5X0P-UqiVia1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a462ec-03f6-49f0-c718-b8c9a3b9d1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "txt = 'Sentence splitting is the process of dividing text into sentences'\n",
        "try:\n",
        "  assert find_similar_words(txt) == ['sentence', 'sentences']\n",
        "  print('passed')\n",
        "except AssertionError:\n",
        "  print('failed')\n",
        "  print(find_similar_words(txt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An8ReNtZbU9B"
      },
      "source": [
        "You can find an additional fuzzy matching component for SpaCy here:\n",
        "https://github.com/gandersen101/spaczz\n",
        "\n",
        "There are many different libraries for this, such as tfidf-matcher, fuzzywuzzy, rapidfuzz, etc.\n",
        "\n",
        "For simplicity, we will not touch them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A8htU4Vyrv4"
      },
      "source": [
        "## Part III. Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArQl_x1lzbJp"
      },
      "source": [
        "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
        "\n",
        "For this task it is better to use either **NLTK** or **SpaCy** libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOe2NSso-_2e"
      },
      "source": [
        "### 3.1 NLTK Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qGGNZmfb0OF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f2880e4-61ce-49a2-a12e-4aba7b18823f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "cfdsfLWy0Rlt"
      },
      "outputs": [],
      "source": [
        "def nltk_ner(txt: str) -> list:\n",
        "\n",
        "  out = []\n",
        "\n",
        "  for sent in nltk.sent_tokenize(txt):\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "      if hasattr(chunk, 'label'):\n",
        "          out.append(f\"{chunk.label()} {' '.join(c[0] for c in chunk)}\")\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "P48ljJYl3Lw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8faaf1-6bf2-4f94-cc87-1557732fa226"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PERSON Boris', 'PERSON Johnson', 'ORGANIZATION Chequers']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "txt = \"Boris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday's party was a 'private event'.\"\n",
        "nltk_ner(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTFdEgo8WQbv"
      },
      "outputs": [],
      "source": [
        "# !pip install svgling\n",
        "nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(nltk.sent_tokenize(txt)[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80aW_vC2lKG"
      },
      "source": [
        "### 3.2 SpaCy Approach\n",
        "Your task.\n",
        "\n",
        "Repeat the same procedure using the Spacy library. Compare the results of both functions. What can you say about it?\n",
        "\n",
        "list of all useful properties:  [Doc](https://spacy.io/api/doc#ents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "k7ZuIX2b0OsT"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "0jZNLxNQ4Juz"
      },
      "outputs": [],
      "source": [
        "def spacy_ner(txt: str) -> list: # output should be like ['label1 word1', 'label2 word2', ...]\n",
        "  doc = nlp(txt)\n",
        "  allowed = {\"PERSON\", \"ORDINAL\", \"DATE\"}\n",
        "  out = []\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ in allowed:\n",
        "      out.append(f\"{ent.label_} {ent.text}\")\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Sib8gDjk0skw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f820aa-0017-441d-b862-02776de85da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "txt = \"Boris Johnson skipped out on his second emergency meeting in a row today and tomorrow he is hosting a lavish goodbye bash at Chequers. A spokesman said Sunday's party was a 'private event'.\"\n",
        "try:\n",
        "  assert spacy_ner(txt) == ['PERSON Boris Johnson', 'ORDINAL second', 'DATE today', 'DATE tomorrow', 'DATE Sunday']\n",
        "  print('passed')\n",
        "except AssertionError:\n",
        "  print('failed')\n",
        "  print(spacy_ner(txt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy is much more advanced which trained on larger datasets, and tends to capture almost all the labels (multiword names, date, ordinal, location)\n",
        "It also over predicts extra entities\n",
        "\n",
        "NLTK is more conservative and less accurate which splits multiword names incorrectly\n",
        "mislabels entities (Chequers as ORGANIZATION instead of GPE)\n",
        "misses dates like today and tomorrow entirely"
      ],
      "metadata": {
        "id": "3M1Ju7pZiqcN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZpX0e4U5ucI"
      },
      "source": [
        "SpaCy can also render named entities so you can check yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "gCUQQzcB7R7K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c9c6b9c8-c456-4e61-964c-c6324cd1558a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Boris Johnson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " skipped out on his \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    second\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " emergency meeting in a row \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    today\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    tomorrow\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " he is hosting a lavish goodbye bash at \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Chequers\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". A spokesman said \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sunday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "'s party was a 'private event'.</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(nlp(txt), jupyter=True, style='ent')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}